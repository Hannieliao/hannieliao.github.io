# üìù Publications 
## üéô Audio Generation

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCAI 2024</div><img src='images/baton.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**BATON: Aligning Text-to-Audio Model with Human Preference Feedback** \\
**Huan Liao**<sup>‚òÖ</sup>, Haonan Han<sup>‚òÖ</sup>, Kai Yang, Tianjiao Du, Rui Yang, Zunnan Xu, Qinmei Xu, Jingquan Liu, Jiasheng Lu, Xiu Li<sup>‚Ä†</sup>

[**[Project]**](https://baton2024.github.io/) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>
[**[Paper]**](https://www.ijcai.org/proceedings/2024/0502.pdf) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>
[**[Dataset&Code]**](https://github.com/Hannieliao/Baton) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>

- **The first text-to-audio (TTA) system finetuned from human preference feedback.**
- Curated a dataset containing both prompts and the corresponding generated audio, annotated based on human feedback.
- Addressed the audio event semantic omission and temporal disarray with a weighted preference strategy
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICME 2024</div><img src='images/temporal.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Controllable Text-to-Audio Generation with Training-Free Temporal Guidance Diffusion** \\
Tianjiao Du, Jun Chen, Jiasheng Lu, Qinmei Xu, **Huan Liao**, Yupeng Chen, Zhiyong Wu<sup>‚Ä†</sup>

[**[Paper]**](https://ieeexplore.ieee.org/abstract/document/10687830) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>
- Training-free approach for controllable TTA generation based on the location and duration of corresponding sound events.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ARXIV 2024</div><img src='images/foley.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Rhythmic Foley: A Framework for Seamless Audio-Visual Alignment in Video-to-Audio Synthesis** \\
Zhiqi Huang<sup>‚òÖ</sup> Dan Luo<sup>‚òÖ</sup> Jun Wang<sup>‚Ä†</sup> **Huan Liao** Zhiheng Li<sup>‚Ä†</sup> Zhiyong Wu<sup>‚Ä†</sup>

[**[Project]**](https://angelalilyer.github.io/RhythmicFoley/) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>
[**[Paper]**](https://arxiv.org/abs/2409.08628) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>
- An innovative framework for video-to-audio synthesis, characterized by semantic integrity and precise beat point synchronization.
</div>
</div>

## üßô 3D/Motion Generation
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ARXIV 2024</div><img src='images/reparo.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**REPARO: Compositional 3D Assets Generation with Differentiable 3D Layout Alignment** \\
Haonan Han<sup>‚òÖ</sup>, Rui Yang<sup>‚òÖ</sup>, **Huan Liao**<sup>‚òÖ</sup>, Jiankai Xing, Zunnan Xu, Xiaoming Yu, Junwei Zha, Xiu Li<sup>‚Ä†</sup>, Wanhua Li<sup>‚Ä†</sup>

[**[Project]**](https://reparo2024.github.io/) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>
[**[Paper]**](https://arxiv.org/pdf/2405.18525) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>
[**[Code]**](https://github.com/VincentHancoder/REPARO?tab=readme-ov-file) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>
- A novel approach for compositional 3D asset generation from single images.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ARXIV 2024</div><img src='images/atom.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**AToM: Aligning Text-to-Motion Model at Event-Level with GPT-4Vision Reward** \\
Haonan Han<sup>‚òÖ</sup>, Xiangzuo Wu<sup>‚òÖ</sup>, **Huan Liao**<sup>‚òÖ</sup>, Zunnan Xu, Ronghui Li, Yachao Zhang<sup>‚Ä†</sup>, Xiu Li<sup>‚Ä†</sup>

[**[Project]**](https://atom-motion.github.io/) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>
[**[Paper]**](https://arxiv.org/pdf/2411.18654) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>
[**[Code]**](https://github.com/VincentHancoder/AToM) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:qjMakFHDy7sC'></span></strong>
- Enhances the event-level alignment between generated motion and text prompts by leveraging reward from GPT-4Vision.
</div>
</div>
